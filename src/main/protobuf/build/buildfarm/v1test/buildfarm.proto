syntax = "proto3";

package build.buildfarm.v1test;

import "google/api/annotations.proto";
import "build/bazel/remote/execution/v2/remote_execution.proto";
import "google/longrunning/operations.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";
import "google/rpc/status.proto";

option go_package = "buildfarm";
option java_package = "build.buildfarm.v1test";
option java_multiple_files = true;
option java_outer_classname = "OperationQueueProto";

// The Admin API is used to perform administrative functions on Buildfarm
// infrastructure
service Admin {
  rpc TerminateHost(TerminateHostRequest) returns (google.rpc.Status) {
    option (google.api.http) = {
      get : "/v1test/admin:terminateHost"
    };
  }

  rpc StopContainer(StopContainerRequest) returns (google.rpc.Status) {
    option (google.api.http) = {
      get : "/v1test/admin:stopContainer"
    };
  }

  rpc GetHosts(GetHostsRequest) returns (GetHostsResult) {
    option (google.api.http) = {
      get : "/v1test/admin:getHosts"
    };
  }

  rpc GetClientStartTime(GetClientStartTimeRequest)
      returns (GetClientStartTimeResult) {
    option (google.api.http) = {
      get : "/v1test/admin:getClientStartTime"
    };
  }

  rpc ScaleCluster(ScaleClusterRequest) returns (google.rpc.Status) {
    option (google.api.http) = {
      get : "/v1test/admin:scaleCluster"
    };
  }

  rpc ReindexCas(ReindexCasRequest) returns (ReindexCasRequestResults) {
    option (google.api.http) = {
      get : "/v1test/admin:reindexCas"
    };
  }

  rpc ShutDownWorkerGracefully(ShutDownWorkerGracefullyRequest)
      returns (ShutDownWorkerGracefullyRequestResults) {
    option (google.api.http) = {
      get : "/v1test/admin:deregisterWorker"
    };
  }

  rpc DisableScaleInProtection(DisableScaleInProtectionRequest)
      returns (DisableScaleInProtectionRequestResults) {
    option (google.api.http) = {
      get : "/v1test/admin:disableScaleInProtection"
    };
  }
}

message ShutDownWorkerGracefullyRequest {
  string instance_name = 1;

  string worker_name = 2;
}

message ShutDownWorkerGracefullyRequestResults {}

message DisableScaleInProtectionRequest { string instance_name = 1; }

message DisableScaleInProtectionRequestResults {}

message ReindexCasRequest {
  string instance_name = 1;

  // format: ip:port
  string host_id = 2;
}

message ReindexCasRequestResults {
  int64 removedHosts = 1;

  int64 removedKeys = 2;

  int64 totalKeys = 3;
}

message TerminateHostRequest {
  string instance_name = 1;

  string host_id = 2;
}

message StopContainerRequest {
  string instance_name = 1;

  string host_id = 2;

  string container_name = 3;
}

message GetHostsRequest {
  string filter = 1;

  int32 age_in_minutes = 2;

  string status = 3;
}

message GetHostsResult {
  int64 num_hosts = 1;

  repeated Host hosts = 2;
}

message Host {
  int64 host_num = 1;

  string host_id = 2;

  string ip_address = 3;

  google.protobuf.Timestamp launch_time = 4;

  string state = 5;

  string dns_name = 6;

  int64 uptime_minutes = 7;

  int64 num_cores = 8;

  string type = 9;

  string lifecycle = 10;
}

message GetClientStartTimeRequest {
  string instance_name = 1;
  repeated string host_name = 2;
}

message GetClientStartTimeResult {
  repeated GetClientStartTime client_start_time = 1;
}

message GetClientStartTime {
  string instance_name = 1;
  google.protobuf.Timestamp client_start_time = 2;
}

message ScaleClusterRequest {
  string scale_group_name = 1;

  int32 min_hosts = 2;

  int32 max_hosts = 3;

  int32 target_hosts = 4;

  int32 target_reserved_hosts_percent = 5;
}

// The OperationQueue API is used internally to communicate with Workers
service OperationQueue {
  rpc Take(TakeOperationRequest) returns (QueueEntry) {
    option (google.api.http) = {
      get : "/v1test/{instance_name=**}/operation:take"
    };
  }

  rpc Put(google.longrunning.Operation) returns (google.rpc.Status) {
    option (google.api.http) = {
      post : "/v1test/{instance_name=**}/operation:put"
      body : "*"
    };
  }

  rpc Poll(PollOperationRequest) returns (google.rpc.Status) {
    option (google.api.http) = {
      get : "/v1test/{instance_name=**}/operation:poll"
    };
  }

  rpc Status(BackplaneStatusRequest) returns (BackplaneStatus) {
    option (google.api.http) = {
      get : "/v1test/{instance_name=**}/operation:status"
    };
  }
}

message BackplaneStatusRequest { string instance_name = 1; }

message TakeOperationRequest {
  // The instance of the execution system to operate against. A server may
  // support multiple instances of the execution system (with their own workers,
  // storage, caches, etc.). The server MAY require use of this field to select
  // between them in an implementation-defined fashion, otherwise it can be
  // omitted.
  string instance_name = 1;

  // The platform features available for the execution environment. The server
  // MAY choose to execute the action on any worker satisfying the requirements,
  // so the client SHOULD ensure that running the action on any such worker will
  // have the same result.
  build.bazel.remote.execution.v2.Platform platform = 5;
}

message PollOperationRequest {
  // The operation name in question
  string operation_name = 2;

  // The current state of the worker
  build.bazel.remote.execution.v2.ExecutionStage.Value stage = 3;
}

// Publish GRPC metrics to promethesus for each server endpoint. This is used
// for SLAs at API boundaries. You may want something similar at the proxy level
// to see frontend/backend performance.
message GrpcPrometheusMetrics {
  // Whether to enable this metric gathering at all. If disabled, there should
  // be no related metrics registered in prometheus.
  bool enabled = 1;

  // These metrics are more expensive to calculate. If disabled, you only
  // receive core metrics. Core metrics include send/receive totals tagged
  // with return codes.  No latencies.
  bool provide_latency_histograms = 2;
}

message BuildFarmServerConfig {
  InstanceConfig instance = 1;

  ActionCacheAccessPolicy ac_policy = 12;

  int32 port = 2;

  int32 execute_keepalive_after_seconds = 3;

  MetricsConfig metrics_config = 4;

  AdminConfig admin_config = 5;

  PrometheusConfig prometheus_config = 6;

  BuildEventConfig build_event_config = 7;

  // Absolute path to the PEM file containing both the required certificates and
  // any associated private key.
  optional string ssl_certificate_path = 8;

  // CAS write deadline
  google.protobuf.Duration cas_write_timeout = 9;

  // Bytestream upload blob deadline
  google.protobuf.Duration bytestream_timeout = 10;

  // Publish GRPC metrics to promethesus for each server endpoint.
  // We use this for SLAs at the API boundary.
  // You may want something similar at the proxy level to compare
  // frontend/backend performance.
  GrpcPrometheusMetrics grpc_metrics = 11;
}

message MetricsConfig {
  string metrics_destination = 1;

  string cluster_id = 2;

  oneof type {
    AwsMetricsConfig aws_metrics_config = 3;
    GcpMetricsConfig gcp_metrics_config = 4;
    LogMetricsConfig log_metrics_config = 5;
  }
}

message AwsSecret {

  string access_key_id = 1;
  string secret_key = 2;
}

message S3BucketConfig {

  string region = 1;

  string name = 2;

  int32 max_connections = 3;

  string secret_name = 4;
}

message AwsMetricsConfig {
  string operations_metrics_topic = 1;

  int32 sns_client_max_connections = 2;

  string secret_name = 3;

  string region = 4;
}

message GcpMetricsConfig { string operations_metrics_topic = 1; }

message LogMetricsConfig { string log_level = 1; }

message PrometheusConfig { int32 port = 1; }

message BuildEventConfig { bool record_events = 1; }

message AdminConfig {
  string deployment_environment = 1;

  string cluster_id = 4;

  string cluster_endpoint = 6;

  oneof type {
    AwsAdminConfig aws_admin_config = 2;
    GcpAdminConfig gcp_admin_config = 3;
  }

  bool enable_graceful_shutdown = 5;
}

message AwsAdminConfig { string region = 1; }

message GcpAdminConfig {}

message MemoryCASConfig {
  // limit for CAS total content size
  int64 max_size_bytes = 1;
}

message GrpcCASConfig {
  // CAS resources must contain instance names
  string instance_name = 1;

  // grpc endpoint supporting CAS/BS
  string target = 2;
}

message FilesystemCASConfig {
  // path to cached files from CAS
  // if relative, is made relative to root
  string path = 1;

  // limit for contents of files retained
  // from CAS in the cache
  int64 max_size_bytes = 2;

  // limit for contents of a single file retained
  // from CAS in the cache
  int64 max_entry_size_bytes = 3;

  // exponential levels of blob bucketing by digest
  // if digest is afcd0123, with 2 here, it will live in af/cd/af0123
  // 0 (default) here will put all entries at the cache root
  int32 hex_bucket_levels = 5;

  // whether the file directories bidirectional mapping should be stored in
  // memory (HashMap) or in sqlite
  bool file_directories_index_in_memory = 4;
}

message FilesystemACConfig {
  // path to action cache files from AC
  string path = 1;
}

message FuseCASConfig {
  // used to identify the backing store
  string name = 1;
}

message ContentAddressableStorageConfig {
  oneof type {
    MemoryCASConfig memory = 1;
    GrpcCASConfig grpc = 2;
    FilesystemCASConfig filesystem = 3;
    FuseCASConfig fuse = 4;
  }

  // whether the transient data on the worker should be loaded into the CAS on
  // worker startup
  bool skip_load = 5;
}

message DelegateCASConfig {}

message GrpcACConfig {
  // AC requests can contain instance names
  string instance_name = 1;

  // grpc endpoint supporting AC
  string target = 2;
}

message ActionCacheConfig {
  oneof type {
    DelegateCASConfig delegate_cas = 1;
    FilesystemACConfig filesystem = 3;
    GrpcACConfig grpc = 2;
  }
}

message MemoryInstanceConfig {
  // the limits of the listOperations request
  int32 list_operations_default_page_size = 1;
  int32 list_operations_max_page_size = 2;

  // the limits of the getTree request
  int32 tree_default_page_size = 3;
  int32 tree_max_page_size = 4;

  // timeout after dispatch before which executing,
  // complete or an operation poll must be received, or
  // the operation is considered lost on a worker and is
  // requeued
  google.protobuf.Duration operation_poll_timeout = 5;

  // delay after timeout when executing before which
  // completed must be received, or the operation is
  // considered lost on a worker and is requeued
  google.protobuf.Duration operation_completed_delay = 6;

  // default timeout for actions
  // if a timeout is unspecified for an action, this value
  // is imposed on it, after which the operation will be
  // cancelled
  google.protobuf.Duration default_action_timeout = 7;

  // maximum selectable timeout
  // a maximum threshold for an action's specified timeout,
  // beyond which an action will be rejected for execution
  google.protobuf.Duration maximum_action_timeout = 8;

  ContentAddressableStorageConfig cas_config = 9;

  ActionCacheConfig action_cache_config = 10;
}

message ProvisionedQueue {
  string name = 1;

  bool allow_unmatched = 3;

  build.bazel.remote.execution.v2.Platform platform = 2;
}

message ProvisionedQueuesConfig { repeated ProvisionedQueue queues = 1; }

enum QueueType {
  // the type of the redis queue
  REGULAR = 0;
  PRIORITY = 1;
};

message RedisShardBackplaneConfig {
  // the uri endpoint of the redis target. This must be a single host entry
  // with cluster discoverability enabled if the redis service is configured
  // for cluster operation. If the endpoint is a singleton redis node, cluster
  // adaptive behaviors, specifically queue balancing, will be emulated.
  string redis_uri = 1;

  // the all redis nodes addreses that the JedisCluster can connect to
  // ideally that's not needed if you have Sentinel set up on some port
  // but if don't have, probably you will achive better
  // stability/discoverability that way
  repeated string redis_nodes_uris = 36;

  // the password used to authenticate to redis via `auth`
  string redis_password = 34;

  QueueType redis_queue_type = 12;

  // the size of the pool of jedis connections per-cluster-member
  int32 jedis_pool_max_total = 15;

  // the hash of active cas shards. Shards must update a self-reported expiry
  // regularly or face deactivation and excommunication. Advertisements for
  // contents on deactivated workers may be pruned.
  string workers_hash_name = 2;

  // the channel used to communicate cas shard structural changes
  string worker_channel = 25;

  // the prefix of keys which map action keys to action results
  string action_cache_prefix = 3;

  // the expiration time in seconds for action cache entries
  int32 action_cache_expire = 4;

  // the prefix of keys which identify actions that are to be rejected
  // from any request endpoint
  string action_blacklist_prefix = 28;

  // the expiration time in seconds for banned actions
  int32 action_blacklist_expire = 29;

  // the prefix of keys which identify invocations that are to be rejected
  // from any request endpoint
  string invocation_blacklist_prefix = 30;

  // the prefix of keys used to retain current Operation state
  string operation_prefix = 5;

  // the expiration time in seconds for operation keys
  int32 operation_expire = 6;

  // the arrival queue. Contains ExecuteEntry messages
  string pre_queued_operations_list_name = 18;

  // the atomic target list for reliable arrival queue removal
  string processing_list_name = 19;

  // the prefix of the processing list timeout monitor key
  string processing_prefix = 20;

  // the minimum timeout for an operation to be removed from the processing list
  // upon leaving the arrival queue
  int32 processing_timeout_millis = 21;

  // the ready-to-run operation queue. Contains QueueEntry messages
  string queued_operations_list_name = 7;

  // the prefix of the dispatching list timeout monitor key
  string dispatching_prefix = 23;

  // the minimum timeout for an operation to be observable in the dispatched
  // hash upon leaving the ready-to-run queue.
  int32 dispatching_timeout_millis = 24;

  // the hash key of dispatched operations. all operations which have been
  // removed from the ready-to-run queue should make their way here in a timely
  // (per dispatching) fashion.
  string dispatched_operations_hash_name = 8;

  // the prefix of operation update topics
  string operation_channel_prefix = 9;

  // the prefix of cas set keys. A cas set for a key suffixed with a digest
  // address, contain the shards which have advertised storage of the
  // addressed content. This set of valid shards for a digest must intersect
  // with the active shard list (workers)
  string cas_prefix = 10;

  // the expiration time in seconds for cas sets
  int32 cas_expire = 11;

  // whether or not to cache the cas set in memory
  bool cache_cas = 35;

  // whether to listen to the backplane communication streams for shard
  // lifecycle events and watched operations. This is required for failsafe
  // operation below and realtime notification for watched operations. This is
  // nearly required for schedulers, and may be useful elsewhere
  bool subscribe_to_backplane = 26;

  // whether to run the failsafe operation monitor, which establishes watchdogs
  // on watched operations, to ensure they are in a known or recently refreshed
  // state, and guaranteeing a watch's termination on expiry
  // This monitor also effects processing/dispatching expirations globally,
  // required to ensure queue reliability. This is recommended for schedulers,
  // meaningless for others
  bool run_failsafe_operation = 27;

  // the maximum size allowed for the ready-to-run queue before rejection
  int32 max_queue_depth = 16;

  // the maximum size allowed for the arrival queue before rejection
  int32 max_pre_queue_depth = 17;

  // the provisioned queue definitions for platform partitioning
  ProvisionedQueuesConfig provisioned_queues = 31;

  // connection and socket read timeout for jedis
  int32 timeout = 32;

  // maximum number of retries in a cluster
  int32 max_attempts = 33;
}

message ShardInstanceConfig {
  bool run_dispatched_monitor = 1;

  int32 dispatched_monitor_interval_seconds = 2;

  bool run_operation_queuer = 3;

  oneof backplane {
    RedisShardBackplaneConfig redis_shard_backplane_config = 4;
  }

  bool ensure_outputs_present = 12;

  int64 max_entry_size_bytes = 5;

  int32 max_cpu = 9;

  // maximum selectable timeout
  // a maximum threshold for an action's specified timeout,
  // beyond which an action will be rejected for execution
  google.protobuf.Duration maximum_action_timeout = 7;

  // default timeouts on grpc requests made by instances
  google.protobuf.Duration grpc_timeout = 8;

  // Whether the instance should consult a denylist when looking up actions &
  // invocations.
  bool use_deny_list = 10;

  int32 max_requeue_attempts = 11;
}

message ShardWorkerInstanceConfig {
  // whether to stream stdout from processes
  bool stream_stdout = 6;

  // control for process stdout
  CASInsertionPolicy stdout_cas_policy = 7;

  // whether to stream stderr from processes
  bool stream_stderr = 8;

  // control for process stdout
  CASInsertionPolicy stderr_cas_policy = 9;

  // control for process output files
  CASInsertionPolicy file_cas_policy = 10;

  // page size for getTree request
  uint32 tree_page_size = 12;

  // default timeouts on grpc requests made by instances
  google.protobuf.Duration grpc_timeout = 13;
}

message DequeueMatchSettings {

  // whether a worker should accept everything it gets off the queue.
  bool accept_everything = 1;

  // whether a worker should accept platform properties that it does not match
  // with.
  bool allow_unmatched = 2;

  // worker platform used to match operations
  build.bazel.remote.execution.v2.Platform platform = 3;
}

message WorkerCapabilities {

  // This determines whether the shard instance will participate in the
  // backplane and cache it's operation outputs in its own cache file CAS. When
  // omitted from CAS, the worker will send its execution outputs.
  bool cas = 1;

  // This determines whether the shard instance will participate in the
  // execution pool. Workers that don't execute may be used for storage only.
  bool execution = 2;
}

message ShardWorkerConfig {
  ShardWorkerInstanceConfig shard_worker_instance_config = 1;

  int32 port = 2;

  string public_name = 3;

  // base directory for all work being performed
  string root = 4;

  // the sequence of cas into which expirations cascade
  repeated ContentAddressableStorageConfig cas = 27;

  // period of poll requests during execution
  google.protobuf.Duration operation_poll_period = 13;

  // settings used to match with operations
  DequeueMatchSettings dequeue_match_settings = 36;

  // total size of the inline content for
  // action results
  int32 inline_content_limit = 15;

  // input fetch width
  int32 input_fetch_stage_width = 26;

  // execute width
  oneof execute_stage_width_setting {
    int32 execute_stage_width = 16;

    // An offset width used when deriving the execute stage width.
    // Some cores of a machine are reserved for usage other than buildfarm's
    // execution. Provide that offset here when deriving the execute stage width
    // to avoid over saturation.
    int32 execute_stage_width_offset = 40;
  }

  // symlink cas input-only directories
  bool link_input_directories = 17;

  // a list of paths that will not be symlinked as input directories
  repeated string real_input_directories = 41;

  // selected hash function
  build.bazel.remote.execution.v2.DigestFunction.Value digest_function = 18;

  // default timeout for actions
  // if a timeout is unspecified for an action, this value
  // is imposed on it, after which the operation will
  // be killed
  google.protobuf.Duration default_action_timeout = 19;

  // maximum selectable timeout
  // a maximum threshold for an action's specified timeout,
  // beyond which an action will be rejected for execution
  google.protobuf.Duration maximum_action_timeout = 20;

  oneof backplane {
    RedisShardBackplaneConfig redis_shard_backplane_config = 21;
  }

  // available execution policies, will be used to match
  // with an action's platform for selection
  repeated ExecutionPolicy execution_policies = 25;

  reserved 31, 33;

  // default max-cores specification for executions, unlimited if not set
  int32 default_max_cores = 42;

  // limit the available cores to execute_stage_width for unmetered execution if
  // set
  bool limit_global_execution = 29;

  // only allow test executions to respect min-cores > 1, limiting range to 1 ->
  // default_max_cores if not a test
  bool only_multicore_tests = 30;

  // Some actions may need to run in a container specified by the client.  By
  // using exec_properties clients can request the container for buildfarm to
  // pull and use.  This feature is known as "bring your own container".  If the
  // feature is disabled the corresponding exec_properties will be ignored.
  bool allow_bring_your_own_container = 43;

  // user principal name for executions
  string exec_owner = 32;

  WorkerCapabilities capabilities = 38;

  bool error_operation_remaining_resources = 34;

  AdminConfig admin_config = 35;

  PrometheusConfig prometheus_config = 37;

  // a limit on time (in seconds) for input fetch stage to fetch inputs
  int32 input_fetch_deadline = 39;
}

message ShardWorker {
  string endpoint = 1;

  int64 expire_at = 2;
}

message WorkerChange {
  message Add {}

  message Remove {
    string source = 1;

    string reason = 2;
  }

  string name = 1;

  google.protobuf.Timestamp effectiveAt = 2;

  oneof type {
    Add add = 3;

    Remove remove = 4;
  }
}

message OperationChange {
  message Reset {
    google.protobuf.Timestamp expiresAt = 1;

    google.longrunning.Operation operation = 2;
  }

  message Expire { bool force = 1; }

  google.protobuf.Timestamp effectiveAt = 1;

  string source = 2;

  oneof type {
    Reset reset = 3;

    Expire expire = 4;
  }
}

message DispatchedOperation {
  QueueEntry queue_entry = 1;

  int64 requeue_at = 2;
}

message ProfiledQueuedOperationMetadata {
  QueuedOperation queued_operation = 1;

  QueuedOperationMetadata queued_operation_metadata = 2;

  google.protobuf.Duration transformed_in = 3;

  google.protobuf.Duration validated_in = 4;

  google.protobuf.Duration uploaded_in = 5;
}

message ExecuteEntry {
  string operation_name = 1;

  build.bazel.remote.execution.v2.Digest action_digest = 2;

  bool skip_cache_lookup = 3;

  build.bazel.remote.execution.v2.RequestMetadata request_metadata = 4;

  build.bazel.remote.execution.v2.ExecutionPolicy execution_policy = 5;

  build.bazel.remote.execution.v2.ResultsCachePolicy results_cache_policy = 6;

  string stdout_stream_name = 7;

  string stderr_stream_name = 8;

  google.protobuf.Timestamp queued_timestamp = 9;
}

message QueueEntry {
  ExecuteEntry execute_entry = 1;

  build.bazel.remote.execution.v2.Digest queued_operation_digest = 2;

  build.bazel.remote.execution.v2.Platform platform = 3;

  int32 requeue_attempts = 4;
}

message QueueStatus {
  int64 size = 1;

  repeated int64 internal_sizes = 2;

  string name = 3;
}

message OperationQueueStatus {
  repeated QueueStatus provisions = 1;

  int64 size = 2;
}

message LabeledCount {
  string name = 1;

  int64 size = 2;
}

message DispatchedOperationsStatus {

  // the total number of currently dispatched operations
  int64 size = 1;

  // how many of the current dispatched operations are 'build' related
  int64 build_action_amount = 2;

  // how many of the current dispatched operations are 'test' related
  int64 test_action_amount = 3;

  // how many of the current dispatched operations cannot be fetched and
  // determine the type
  int64 unknown_action_amount = 4;

  // count dispatched operations by the queue they originated in
  // for example, "how many cpu vs gpu operations are running?"
  repeated LabeledCount from_queues = 5;

  // count dispatched operations by the tool they originated from
  // for example, "Is anyone using a build tool or bazel version not supported
  // by the main repo?"
  repeated LabeledCount tools = 6;

  // count the kinds of action mnemonics occurring.
  // for example, CppCompile or GoLink.
  repeated LabeledCount action_mnemonics = 7;

  // count the program names being run for the command
  // for example, clang, bash.
  repeated LabeledCount command_tools = 8;

  // count the unique targets being resolved
  repeated LabeledCount target_ids = 9;

  // count the unique configurations being used
  repeated LabeledCount config_ids = 10;

  // platform properties used by the dispatched operations
  repeated LabeledCount platform_properties = 11;

  // how many unique clients are currently being served.
  // This is based on the uniqueness of tool_id for currently dispatched
  // operations.
  int64 unique_clients_amount = 12;

  // how many of the dispatched operations have been requeued.
  // many requeued operations could be a sign of a problem.
  int64 requeued_operations_amount = 13;
}

message BackplaneStatus {
  QueueStatus prequeue = 1;

  OperationQueueStatus operation_queue = 2;

  DispatchedOperationsStatus dispatched_operations = 9;

  repeated string active_workers = 4;

  int64 cas_lookup_size = 5;

  int64 action_cache_size = 6;

  int64 blocked_actions_size = 7;

  int64 blocked_invocations_size = 8;

  int64 fetch_time_ms = 10;

  int64 dispatched_size = 11;
}

message QueuedOperationMetadata {
  build.bazel.remote.execution.v2.ExecuteOperationMetadata
      execute_operation_metadata = 1;

  build.bazel.remote.execution.v2.Digest queued_operation_digest = 2;

  build.bazel.remote.execution.v2.RequestMetadata request_metadata = 3;
}

// A `Tree` contains all the
// [Directory][build.bazel.remote.execution.v2.Directory] protos in a
// single directory Merkle tree, compressed into one message, with a map
// to index the directories.
message Tree {
  // The digest of the root directory in the tree.
  build.bazel.remote.execution.v2.Digest rootDigest = 1;

  map<string, build.bazel.remote.execution.v2.Directory> directories = 2;
}

message QueuedOperation {
  build.bazel.remote.execution.v2.Action action = 1;

  build.bazel.remote.execution.v2.Command command = 2;

  reserved 4;

  Tree tree = 5;
}

message ExecutingOperationMetadata {
  build.bazel.remote.execution.v2.ExecuteOperationMetadata
      execute_operation_metadata = 1;

  build.bazel.remote.execution.v2.RequestMetadata request_metadata = 2;

  int64 started_at = 3;

  string executing_on = 4;
}

message CompletedOperationMetadata {
  build.bazel.remote.execution.v2.ExecuteOperationMetadata
      execute_operation_metadata = 1;

  build.bazel.remote.execution.v2.RequestMetadata request_metadata = 2;

  reserved 3 to 8; // fields now in ExecutedActionMetadata
}

message OperationRequestMetadata {
  string operation_name = 1;

  bool done = 2;

  build.bazel.remote.execution.v2.RequestMetadata request_metadata = 3;

  build.bazel.remote.execution.v2.ExecuteOperationMetadata
      execute_operation_metadata = 4;

  build.bazel.remote.execution.v2.ExecuteResponse execute_response = 5;

  string cluster_id = 6;
}

message InstanceConfig {
  string name = 1;

  build.bazel.remote.execution.v2.DigestFunction.Value digest_function = 2;

  oneof type {
    MemoryInstanceConfig memory_instance_config = 3;
    ShardInstanceConfig shard_instance_config = 4;
  }
}

enum CASInsertionPolicy {
  UNKNOWN = 0;

  ALWAYS_INSERT = 1;

  INSERT_ABOVE_LIMIT = 2;
};

enum ActionCacheAccessPolicy {
  READ_AND_WRITE = 0;

  READ_ONLY = 1;
};

message InstanceEndpoint {
  // target suitable for grpc channel creation: host:port is common
  string target = 1;

  // instance to be used
  string instance_name = 2;

  // deadline for requests in seconds
  int32 deadline_after_seconds = 3;
}

message ExecutionWrapper {
  string path = 1;

  repeated string arguments = 2;
}

// selectable controls for executions
// a universal policy can be specified with an empty name
message ExecutionPolicy {
  string name = 1;

  oneof policy { ExecutionWrapper wrapper = 2; }
}

message WorkerConfig {
  build.bazel.remote.execution.v2.DigestFunction.Value digest_function = 1;

  // endpoint for all cas requests
  InstanceEndpoint content_addressable_storage = 2;

  // endpoint for all action cache requests
  InstanceEndpoint action_cache = 3;

  // endpoint for all operation execution requests
  InstanceEndpoint operation_queue = 4;

  // base directory for all work being performed
  string root = 5;

  // path to cached files from CAS
  // if relative, is made relative to root
  string cas_cache_directory = 6;

  // limit for contents of files retained
  // from CAS in the cache
  int64 cas_cache_max_size_bytes = 7;

  // limit for contents of a single file retained
  // from CAS in the cache
  int64 cas_cache_max_entry_size_bytes = 24;

  // exponential levels of blob bucketing by digest
  // if digest is afcd0123, with 2 here, it will live in af/cd/af0123
  // 0 (default) here will put all entries at the cache root
  int32 cas_cache_hex_bucket_levels = 27;

  // whether the file directories bidirectional mapping should be stored in
  // memory (HashMap) or in sqlite
  bool cas_cache_file_directories_index_in_memory = 28;

  // total size of the inline content for
  // action results
  int32 inline_content_limit = 8;

  // whether to stream stdout from processes
  bool stream_stdout = 9;

  // policy for process stdout
  CASInsertionPolicy stdout_cas_policy = 10;

  // whether to stream stderr from processes
  bool stream_stderr = 11;

  // policy for process stdout
  CASInsertionPolicy stderr_cas_policy = 12;

  // policy for process output files
  CASInsertionPolicy file_cas_policy = 13;

  reserved 14;

  // page size for getTree request
  uint32 tree_page_size = 15;

  // period of poll requests during execution
  google.protobuf.Duration operation_poll_period = 16;

  // initial platform used to match operations
  build.bazel.remote.execution.v2.Platform platform = 17;

  // default platform to populate matched commands
  build.bazel.remote.execution.v2.Platform default_platform = 25;

  // input fetch width
  int32 input_fetch_stage_width = 23;

  // execute width
  oneof execute_stage_width_setting {
    int32 execute_stage_width = 18;

    // An offset width used when deriving the execute stage width.
    // Some cores of a machine are reserved for usage other than buildfarm's
    // execution. Provide that offset here when deriving the execute stage width
    // to avoid over saturation.
    int32 execute_stage_width_offset = 31;
  }

  // symlink cas input-only directories
  bool link_input_directories = 19;

  // default timeout for actions
  // if a timeout is unspecified for an action, this value
  // is imposed on it, after which the operation will
  // be killed
  google.protobuf.Duration default_action_timeout = 20;

  // maximum selectable timeout
  // a maximum threshold for an action's specified timeout,
  // beyond which an action will be rejected for execution
  google.protobuf.Duration maximum_action_timeout = 21;

  // available execution policies, will be used to match
  // with an action's platform for selection
  repeated ExecutionPolicy execution_policies = 22;

  bool error_operation_remaining_resources = 26;

  // user principal name for executions
  string exec_owner = 29;

  // a limit on time (in seconds) for input fetch stage to fetch inputs
  int32 input_fetch_deadline = 30;
}

message TreeIteratorToken {
  repeated build.bazel.remote.execution.v2.Digest directories = 1;
}

message OperationIteratorToken { string operation_name = 1; }

message BlobWriteKey {
  build.bazel.remote.execution.v2.Digest digest = 1;

  string identifier = 2;
}

message OperationTimesBetweenStages {
  google.protobuf.Duration queued_to_match = 1;

  google.protobuf.Duration match_to_input_fetch_start = 2;

  google.protobuf.Duration input_fetch_start_to_complete = 3;

  google.protobuf.Duration input_fetch_complete_to_execution_start = 4;

  google.protobuf.Duration execution_start_to_complete = 5;

  google.protobuf.Duration execution_complete_to_output_upload_start = 6;

  google.protobuf.Duration output_upload_start_to_complete = 7;

  google.protobuf.Duration period = 8;

  int32 operation_count = 9;
}

message StageInformation {
  // name of the stage this message represent, i.e. InputFetchStage
  string name = 1;

  // number of slots configured for this stage
  int32 slots_configured = 2;

  // number of slots used for this stage
  int32 slots_used = 3;
}

message WorkerProfileMessage {
  // number of Entry in CAS
  int64 cas_entry_count = 1;

  // current size of the CAS as sum of entries
  int64 cas_size = 12;

  // configured maximum size of the CAS
  int64 cas_max_size = 13;

  // configured maximum entry size of the CAS
  int64 cas_max_entry_size = 14;

  // number of unreferenced Entry in CAS
  int64 cas_unreferenced_entry_count = 11;

  // number of DirectoryEntry in CAS
  int64 cas_directory_entry_count = 2;

  // number of evicted Entries since last profile request
  int32 cas_evicted_entry_count = 5;

  // the total size of evicted Entries since last profile request
  int64 cas_evicted_entry_size = 6;

  // used for removed fields representing composed used/configured slots
  reserved 7 to 8;

  repeated StageInformation stages = 9;

  repeated OperationTimesBetweenStages times = 10;
}

message WorkerProfileRequest {}

message WorkerListRequest {}

message WorkerListMessage { repeated string workers = 1; }

service WorkerProfile {
  rpc GetWorkerProfile(WorkerProfileRequest) returns (WorkerProfileMessage) {}

  rpc GetWorkerList(WorkerListRequest) returns (WorkerListMessage) {}
}

message PrepareWorkerForGracefulShutDownRequest {
  string instance_name = 1;

  string worker_name = 2;
}

message PrepareWorkerForGracefulShutDownRequestResults {}

service ShutDownWorker {
  rpc PrepareWorkerForGracefulShutdown(PrepareWorkerForGracefulShutDownRequest)
      returns (PrepareWorkerForGracefulShutDownRequestResults) {}
}
