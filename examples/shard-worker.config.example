# the listening port of the shard-worker grpc server
port: 8981

# the host:port of this grpc server, required to be accessible
# by all shard cluster servers
public_name: "localhost:8981"


capabilities {
  # This determines whether the worker is a shard of the cas.
  # When not a shard, it will upload execution outputs.
  cas: true
  
  # This determines whether the shard instance will participate in the execution pool.
  # Workers that don't execute may be used for storage only.
  execution: true
}

# the digest function for this worker, required
# to match out of band between the client and server
# since resource names must be determined on the client
# for a valid upload
digest_function: SHA256

# all content for the operations will be stored under this path
root: "/tmp/worker"

# total size in bytes of inline content for action results
# output files, stdout, and stderr content, in that order
# will be inlined if their cumulative size does not exceed this limit.
inline_content_limit: 1048567 # 1024 * 1024

# the period between poll operations at any stage
operation_poll_period: {
  seconds: 1
  nanos: 0
}

dequeue_match_settings: {

  # key/value set of defining capabilities of this worker
  # all execute requests must match perfectly with workers which
  # provide capabilities
  # so an action with a required platform: { arch: "x86_64" } must
  # match with a worker with at least { arch: "x86_64" } here
  platform: {
    # commented out here for illustrative purposes, a default empty
    # 'platform' is a sufficient starting point without specifying
    # any platform requirements on the actions' side
    ###
    # property: {
    #   name: "key_name"
    #   value: "value_string"
    # }
  }
  accept_everything: True

}

# the worker CAS configuration
cas: {
  # present a filesystem CAS, required to host an exec root
  # the first filesystem CAS in the configuration will be used
  # as the storage for an exec filesystem for an operation
  filesystem: {
    # the local cache location relative to the 'root', or absolute
    path: "cache"

    # limit for contents of files retained
    # from CAS in the cache
    max_size_bytes: 2147483648 # 2 * 1024 * 1024 * 1024

    # limit for content size of files retained
    # from CAS in the cache
    max_entry_size_bytes: 2147483648 # 2 * 1024 * 1024 * 1024

    # whether the file directories bidirectional mapping should be stored in memory (HashMap) or
    # in sqlite
    file_directories_index_in_memory: false

    # create entry files in a directory for each leveled leading byte of the hash
    # the default, unspecified value is 0, to create all digests in the cas path itself
    # for each level > 0, a directory is created for all byte representations, 00-ff, with
    # as many sublevels as requested.
    #
    # This can provide a performance benefit for file access and manipulation for filesystems
    # which have problems with directories with many (e.g. millions) of files in them.
    # The CFC creates these directories upfront, resulting in a delay while each is verified or
    # created. Be cautious with large values, as the subdivision is 256^n
    # hex_bucket_levels: 0
  }

  # whether the transient data on the worker should be loaded into the CAS on worker startup.
  # It can be faster for worker startup to skip loading.
  skip_load: false
}

# another cas entry specification here will provide a fallback
# for the filesystem cas. Any number of consecutive filesystem
# fallbacks may be used, terminated with zero or one of grpc or
# memory types.
#
#cas: {
#  grpc: {
#      instance_name: "external-cas",
#      target: "cas.external.com:1234",
#  }
#}

# the number of concurrently available slots in the execute phase
execute_stage_width: 1

# the number of concurrently available slots in the input fetch phase
input_fetch_stage_width: 1

# a limit on time (in seconds) for input fetch stage to fetch inputs
input_fetch_deadline: 60

# Use an input directory creation strategy which creates a single
# directory tree at the highest level of the input tree containing
# no output paths of any kind, and symlinks that directory into an
# action's execroot, potentially saving large amounts of time
# spent manufacturing the same read-only input hierarchy over
# multiple actions' executions.
link_input_directories: true

# an imposed action-key-invariant timeout used in the unspecified timeout case
default_action_timeout: {
  seconds: 600
  nanos: 0
}

# a limit on the action timeout specified in the action, above which
# the operation will report a failed result immediately
maximum_action_timeout: {
  seconds: 3600
  nanos: 0
}

# prefix command executions with this path
#execution_policies: {
#  name: "test"
#  wrapper: {
#    path: "/path/to/execution/wrapper"
#  }
#}

# A backplane specification hosted with redis cluster
# Fields omitted are expected defaults, and are unused or undesirable on the workers
redis_shard_backplane_config: {
  # The URI of the redis cluster endpoint. This must
  # be a single URI, regardless of the layout of the cluster
  redis_uri: "redis://localhost:6379"

  # The password used to authenticate redis clients, if required
  # redis_password: "mypass"

  # The size of the redis connection pool
  jedis_pool_max_total: 4000

  # The redis key used to store a hash of registered Workers
  # to their registration expiration time. After a worker's
  # registration has expired, they are no longer considered
  # as shards of the CAS
  workers_hash_name: "Workers"

  # A redis pubsub channel key where changes to the cluster
  # membership are announced
  worker_channel: "WorkerChannel"

  # A redis key prefix for all ActionCache entries, suffixed
  # with the action's key and mapping to an ActionResult
  action_cache_prefix: "ActionCache"

  # The ttl maintained for ActionCache entries. This is not
  # refreshed on getActionResult hit
  action_cache_expire: 2419200 # 4 weeks

  # A redis key prefix for all blacklisted actions, suffixed
  # with the action's key hash. An action which is blacklisted
  # should be rejected for all requests where it is identified via
  # its RequestMetadata
  # To meet API standards, a request which matches this condition
  # receives a transient UNAVAILABLE response, which, in bazel's
  # case, can induce a fallback to non-remote recovery, rather
  # than a catastrophic failure.
  action_blacklist_prefix: "ActionBlacklist"

  # The ttl maintained for action_blacklist entries.
  action_blacklist_expire: 3600 # 1 hour

  # A redis key prefix for all blacklisted invocations, suffixed
  # with the tool invocation id. Requests on behalf of an invocation
  # which is blacklisted should be rejected where it is identified via
  # its RequestMetadata
  # To meet API standards, a request which matches this condition
  # receives a transient UNAVAILABLE response, which, in bazel's
  # case, can induce a fallback to non-remote recovery, rather
  # than a catastrophic failure.
  invocation_blacklist_prefix: "InvocationBlacklist"

  # A redis key prefix for all Operations, suffixed with the
  # operation's name and mapping to an Operation which reflects
  # the cluster perceived state of that Operation
  operation_prefix: "Operation"

  # The ttl maintained for all Operations, updated on each
  # modification
  operation_expire: 604800 # 1 week

  # The redis key used to store a list of QueueEntrys
  # awaiting execution by workers. These are queued
  # by an operation_queuer agent, and dequeued by a worker.
  # Redis keyspace manipulation is used here to support multi-
  # key commands.
  # The string contained within {} must match that of
  # dispatching_list_name.
  queued_operations_list_name: "{Execution}:QueuedOperations"

  # A redis key prefix for operations which are being dequeued
  # from the ready-to-run queue. The key is suffixed with the
  # operation name and contains the expiration time in epoch
  # milliseconds after which the operation is considered lost.
  dispatching_prefix: "Dispatching"

  # The delay in milliseconds used to populate dispatching operation
  # entries
  dispatching_timeout_millis: 10000

  # The redis key of a hash of operation names to the worker
  # lease for its execution. Entries in this hash are monitored
  # by the dispatched_monitor for expiration, and the worker
  # is expected to extend a lease in a timely fashion to indicate
  # continued handling of an operation.
  dispatched_operations_hash_name: "DispatchedOperations"

  # A redis pubsub channel prefix suffixed by an operation name
  # where updates and keepalives are transmitted as it makes its
  # way through the various processing elements of the cluster.
  operation_channel_prefix: "OperationChannel"

  # A redis key prefix suffixed with a blob digest that maps to a
  # set of workers which advertise that blob's availability.
  # This set must be intersected with the set of active worker
  # leases to be considered meaningful.
  cas_prefix: "ContentAddressableStorage"

  # The ttl maintained for cas entries. This is not refreshed on
  # any read access of the blob.
  cas_expire: 604800 # 1 week

  # Enable an agent in the backplane client which subscribes
  # to worker_channel and operation_channel events. If this is
  # disabled, the responsiveness of watchers is reduced and the
  # CAS is reduced.
  # When in doubt, leave this enabled.
  subscribe_to_backplane: true

  ## Specify a queue that supports min/max cpu core specification
  provisioned_queues: {
    queues: {
      name: "cpu"

      platform: {
        # Any specification (including non-specification) of min/max-cores
        # will be allowed to support cpu controls and worker resource
        # delegation.
        properties: {
          name: "min-cores"
          value: "*"
        }
        properties: {
          name: "max-cores"
          value: "*"
        }
      }
    }
  }

  # Redis connection and read timeout values in milliseconds (defaults: 2000, minimum value: 2000)
  #timeout: 20000
  # Redis maximum attempts (default: 5, minimum value: 5)
  #max_attempts: 10
}

# Create exec trees containing directories that are owned by
# this user
# The default (empty) value does not change the owner
#exec_owner: "nobody"
exec_owner: ""
